{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total points for this HW: 100.\n",
    "Due date: 02/20/2020 (at the start of class)\n",
    "\n",
    "Please note: Copying and pasting other people's work is absolutely prohibited.  Any such cases will be reported to CUSP's education team and severely punished. Discussion is encouraged, and feel free to exchange ideas with your classmates, but please write your own code and do your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 1: Accuracy and interpretability (10 pts)\n",
    "\n",
    "a) Describe a real-world prediction problem using urban data for which _interpretability_ of your models and results is essential, and for which it might be preferable to use decision trees rather than random forests.  Argue why this is the case. (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interpretability is paramount in a lot of real-world machine learning problems. Those problems were understanding the processes is as important as reaching the result.  For example, helping the city assess the risk surrounding its building stock by predicting which buildings are prone to falling. Such a problem might include looking into features such as the building age, construction type, and any symptoms the building is showing (i.e. cracking, unsymmetrical facade, …). Technical experts in the field of building restoration will need to go through these variables sequentially, answer questions pertaining to why values of these variables pose a high risk and explain the consequences of this risk to city officials that lack domain expertise before the city can make a decision to demolish a building. Therefore, the need to understand each step requires a comprehensible model that is closer to a decision tree than to a complex of trees working together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe a real-world prediction problem using urban data for which _accuracy_ is paramount and interpretability may be less important, and for which it might be preferable to use random forests rather than decision trees.  Argue why this is the case. (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Two examples for accuracy gaining high importance are cases exhibiting high risk or cases were interpretability is not required.\n",
    "\n",
    "> Real-world predictions that bear high risk, are problems where accuracy is essential. Disease classification problems are ones that require high accuracy. For example, predicting the presence or absence of diabetes based on various medical/health symptoms. In disease predicting algorithms, accuracy is paramount since classifying a healthy person as sick would result in unnecessary testing, treatments, and medications. \n",
    "\n",
    "> Moreover, real-world problems were biases are minimal and the risk for discrimination is low are problems where interpretability is less important and random forests are a good solution. For example, a classification model that uses remotely sensed data to predict land cover. The interpretability of the process is not as important as accurately mapping the distribution of trees, grass, and water in an aerial image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Let's imagine that you want to try to get the best of both worlds (accuracy _and_ interpretability).  So you decide to start by learning a random forest classifier.  Describe at least one way of getting some interpretability out of the model by post-processing.  You could either pick a method from the literature (e.g., Domingos's work on combining multiple models or some method of computing variable importance), or come up with your own approach (doesn't have to be ground-breaking, but feel free to be creative!) (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ensemble methods that are based on learning multiple models, improve accuracy and stability at the expense of interpretability. One way of regaining some understanding of a complex ensemble method such as a random forest classier is one proposed by Pedro Domingos. Domingos’s post-processing method proposes the production of a single tree that combines the inherent comprehensibility of single trees as well as the accuracy of multiple trees ensembles. After learning a random forest classifier, the predictions generated by the latter are then fed along with the original training set to a base decision tree to train it. This intends to produce a simple tree with high accuracy.\n",
    "\n",
    "> Moreover, reducing the number of features in a dataset using feature selection has the potential of increasing interpretability. One method of variable selection is the measure of variable importance. Put simply, variable importance permutates each variable and checks for any change in information gain after permutation. If no change occurs it means that this variable is dispensable. \n",
    "\n",
    "> Another possible way to gain back some interpretability is to try to find one tree in a random forest model that has the ability to predict values closest to what the whole model would predict. This could be the tree with a similar performance score as the whole model. We should be aware of the possibility of overfitting in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2: Build a decision tree for classification, step by step, following the lecture notes. Note that the dataset has been slightly modified, so you will get a different tree than the one shown in the lecture notes.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>HP</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>175</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>139</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>190</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>170</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MPG  cylinders   HP   weight\n",
       "0   good          4   75    light\n",
       "1    bad          6   90   medium\n",
       "2    bad          4  110   medium\n",
       "3    bad          8  175  weighty\n",
       "4    bad          6   95   medium\n",
       "5    bad          4   94    light\n",
       "6    bad          4   95    light\n",
       "7    bad          8  139  weighty\n",
       "8    bad          8  190  weighty\n",
       "9    bad          8  145  weighty\n",
       "10   bad          6  100   medium\n",
       "11  good          4   92   medium\n",
       "12   bad          6  100  weighty\n",
       "13   bad          8  170  weighty\n",
       "14  good          4   89   medium\n",
       "15  good          4   65    light\n",
       "16   bad          6   85   medium\n",
       "17  good          4   81    light\n",
       "18   bad          6   95   medium\n",
       "19   bad          4   93    light"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from StringIO import StringIO\n",
    "thefile = StringIO('MPG,cylinders,HP,weight\\ngood,4,75,light\\nbad,6,90,medium\\nbad,4,110,medium\\nbad,8,175,weighty\\nbad,6,95,medium\\nbad,4,94,light\\nbad,4,95,light\\nbad,8,139,weighty\\nbad,8,190,weighty\\nbad,8,145,weighty\\nbad,6,100,medium\\ngood,4,92,medium\\nbad,6,100,weighty\\nbad,8,170,weighty\\ngood,4,89,medium\\ngood,4,65,light\\nbad,6,85,medium\\ngood,4,81,light\\nbad,6,95,medium\\nbad,4,93,light')\n",
    "df = pd.read_csv(thefile)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please use numpy and pandas to do the computation for parts a) through f).  Do not use an existing decision tree implementation like sklearn for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Start with the entire dataset and find the most common MPG value. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad     15\n",
      "good     5\n",
      "Name: MPG, dtype: int64\n",
      "\n",
      "The most common MPG value is: bad\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "print df.MPG.value_counts()\n",
    "print\n",
    "print 'The most common MPG value is: {}'.format(df.MPG.value_counts().argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InformationGain(goodY,badY,goodN,badN):\n",
    "    def F(X,Y):\n",
    "        val1 = X*np.log2(1.*(X+Y)/X) if X>0 else 0\n",
    "        val2 = Y*np.log2(1.*(X+Y)/Y) if Y>0 else 0\n",
    "        return val1+val2\n",
    "    return (F(goodY+goodN,badY+badN)-F(goodY,badY)-F(goodN,badN)) / (goodY+goodN+badY+badN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Enumerate all the possible binary questions you could ask for each discrete-valued variable.  For each such split, compute the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, and compute the information gain using the provided function above. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'good']\n"
     ]
    }
   ],
   "source": [
    "print list(set(df.MPG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_var_IG(dataframe, attributes, target):\n",
    "    \n",
    "    ''' a function that takes in the data, a list of attributes and the target variable. \n",
    "    the function enumerates through all the possible binary questions for each discrete-valued variable.  \n",
    "    For each such split, it computes the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, \n",
    "    and compute the information gain using the previously defined InformationGain function'''\n",
    "    \n",
    "    # print the header for the results\n",
    "    print '-' * 60\n",
    "    print '{:<10s}{:>15s}{:>12s}{:>20s}'.format('Split', 'Group Y', 'Group N', 'Information Gain')\n",
    "    print '-' * 60\n",
    "    \n",
    "    for att in attributes:\n",
    "        for value in set(dataframe[att]):\n",
    "            Y, N = [], []\n",
    "            for index, row in dataframe.iterrows():\n",
    "                if row[att] == value:\n",
    "                    Y.append(index)\n",
    "                else:\n",
    "                    N.append(index)\n",
    "                    \n",
    "            goodY = len([i for i in Y if df[target][i] == list(set(df[target]))[1]]) \n",
    "            badY = len([i for i in Y if df[target][i] == list(set(df[target]))[0]])\n",
    "            goodN = len([i for i in N if df[target][i] == list(set(df[target]))[1]])\n",
    "            badN = len([i for i in N if df[target][i] == list(set(df[target]))[0]])\n",
    "            \n",
    "            print '{}={}?{:>7d}+/{}-{:>8d}+/{}-{:>14f}'.format(\n",
    "                                                        att, value, goodY, badY, goodN, badN, \n",
    "                                                        InformationGain(goodY,badY,goodN,badN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Split             Group Y     Group N    Information Gain\n",
      "------------------------------------------------------------\n",
      "cylinders=8?      0+/5-       5+/10-      0.122556\n",
      "cylinders=4?      5+/4-       0+/11-      0.365294\n",
      "cylinders=6?      0+/6-       5+/9-      0.153078\n",
      "weight=light?      3+/3-       2+/12-      0.097107\n",
      "weight=medium?      2+/6-       3+/9-      0.000000\n",
      "weight=weighty?      0+/6-       5+/9-      0.153078\n"
     ]
    }
   ],
   "source": [
    "# all possible splits for descrete variables in the data\n",
    "desc_var_IG(df, ['cylinders', 'weight'], 'MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Enumerate all the possible binary questions you could ask for the real-valued variable HP.  For each such split, compute the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, and compute the information gain using the provided function above. (5 pts) \n",
    "\n",
    "NOTE: if you'd like, you can just use all midpoints between consecutive values of the sorted HP attribute.  You are not required to exclude provably suboptimal questions like we did in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_var_IG(dataframe, attributes, target):\n",
    "    \n",
    "    ''' a function that takes in the data, a list of attributes and the target variable. \n",
    "    the function enumerates through all the possible binary questions for each real-valued variable.  \n",
    "    For each such split, it computes the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, \n",
    "    and compute the information gain using the previously defined InformationGain function'''\n",
    "    \n",
    "    # sort the values of the attributes and create a list of midpoints\n",
    "    for att in attributes:\n",
    "        att_sorted = list(dataframe[att].sort_values())\n",
    "        print '%s_sorted ='%(att), att_sorted\n",
    "        \n",
    "        att_sorted_mid = [(a+b)/2 for a, b in zip(att_sorted[::2], att_sorted[1::2])]\n",
    "        print '%s_sorted_mid ='%(att), att_sorted_mid\n",
    "\n",
    "        \n",
    "    # print the header for the results\n",
    "    print '-' * 60\n",
    "    print '{:<5s}{:>13s}{:>12s}{:>20s}'.format('Split', 'Group Y', 'Group N', 'Information Gain')\n",
    "    print '-' * 60\n",
    "    \n",
    "\n",
    "    for value in att_sorted_mid:\n",
    "        Y, N = [], []\n",
    "        for index, row in dataframe.iterrows():\n",
    "            if row[att] > value:\n",
    "                Y.append(index)\n",
    "            else:\n",
    "                N.append(index)\n",
    "\n",
    "        goodY = len([i for i in Y if dataframe[target][i] == list(set(dataframe[target]))[1]]) \n",
    "        badY = len([i for i in Y if dataframe[target][i] == list(set(dataframe[target]))[0]])\n",
    "        goodN = len([i for i in N if dataframe[target][i] == list(set(dataframe[target]))[1]])\n",
    "        badN = len([i for i in N if dataframe[target][i] == list(set(dataframe[target]))[0]])\n",
    "\n",
    "        print '{}>{}?{:>7d}+/{}-{:>8d}+/{}-{:>14f}'.format(\n",
    "                                                    att, value, goodY, badY, goodN, badN, \n",
    "                                                    InformationGain(goodY,badY,goodN,badN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP_sorted = [65, 75, 81, 85, 89, 90, 92, 93, 94, 95, 95, 95, 100, 100, 110, 139, 145, 170, 175, 190]\n",
      "HP_sorted_mid = [70, 83, 89, 92, 94, 95, 100, 124, 157, 182]\n",
      "------------------------------------------------------------\n",
      "Split      Group Y     Group N    Information Gain\n",
      "------------------------------------------------------------\n",
      "HP>70?      4+/15-       1+/0-      0.105915\n",
      "HP>83?      2+/15-       3+/0-      0.367103\n",
      "HP>89?      1+/14-       4+/1-      0.365777\n",
      "HP>92?      0+/13-       5+/2-      0.509186\n",
      "HP>94?      0+/11-       5+/4-      0.365294\n",
      "HP>95?      0+/8-       5+/7-      0.223357\n",
      "HP>100?      0+/6-       5+/9-      0.153078\n",
      "HP>124?      0+/5-       5+/10-      0.122556\n",
      "HP>157?      0+/3-       5+/12-      0.068394\n",
      "HP>182?      0+/1-       5+/14-      0.021377\n"
     ]
    }
   ],
   "source": [
    "# all possible splits for real variables in the data\n",
    "real_var_IG(df, ['HP'], 'MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Based on your results for parts b and c, what is the optimal binary split of the data?  Of the two child nodes created by this split, which (if any) would require further partitioning? (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The optimal binary split of the data is on the value '92' of the attribute 'HP' because it resulted in the highest information gain of 0.509186.\n",
    "\n",
    "> All the instances with HP value greater than ‘92’ belong to the same class (13 bad instances in this case). Therefore, there is no need to split further and we can declare this node as a leaf. However, the instances with horse power less than or equal to 92 still include both classes (5 good and 2 bad cars). Therefore, this node will require further splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repeat parts a through d until all training data points are perfectly classified by the resulting tree. (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>HP</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MPG  cylinders  HP  weight\n",
       "0   good          4  75   light\n",
       "1    bad          6  90  medium\n",
       "11  good          4  92  medium\n",
       "14  good          4  89  medium\n",
       "15  good          4  65   light\n",
       "16   bad          6  85  medium\n",
       "17  good          4  81   light"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset the data to get on the values with HP less than or equal to 92\n",
    "df_sub_1 = df[df['HP'] <= 92]\n",
    "df_sub_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Split             Group Y     Group N    Information Gain\n",
      "------------------------------------------------------------\n",
      "cylinders=4?      5+/0-       0+/2-      0.863121\n",
      "cylinders=6?      0+/2-       5+/0-      0.863121\n",
      "weight=light?      3+/0-       2+/2-      0.291692\n",
      "weight=medium?      2+/2-       3+/0-      0.291692\n"
     ]
    }
   ],
   "source": [
    "# all possible splits for descrete variables in the subset data\n",
    "desc_var_IG(df_sub_1, ['cylinders', 'weight'], 'MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-valued attribute HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP_sorted = [65, 75, 81, 85, 89, 90, 92]\n",
      "HP_sorted_mid = [70, 83, 89]\n",
      "------------------------------------------------------------\n",
      "Split      Group Y     Group N    Information Gain\n",
      "------------------------------------------------------------\n",
      "HP>70?      4+/2-       1+/0-      0.076010\n",
      "HP>83?      2+/2-       3+/0-      0.291692\n",
      "HP>89?      1+/1-       4+/1-      0.061743\n"
     ]
    }
   ],
   "source": [
    "# all possible splits for real variables in the subset data\n",
    "real_var_IG(df_sub_1, ['HP'], 'MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    ">Further splitting the data using the value ‘4’ of the attribute 'cylinder' that resulted in the highest information gain will result in two groups each with only one class (good cars are in one group and bad cars are in the other). Therefore, there is no need to split further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Draw or show the final decision tree in a format of your choice.  The decision to make at each step and the predicted value at each leaf node must be clear. (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please make sure you dowloaded the image (tree.jpg) and have it in the same directory with this notebook to be able to view the image below.**\n",
    "![alt text](tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Classify each of the following four vehicles as having \"good\" or \"bad\" fuel efficiency (miles per gallon).  Do this by hand using the tree structure learned in part f. (4 pts)\n",
    "\n",
    "> good,4,70,light\n",
    "\n",
    "> bad,6,113,medium\n",
    "\n",
    "> bad,6,83,weighty\n",
    "\n",
    "> bad,4,95,weighty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3, Predicting burden of disease （40 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>FrxnPeaceIn10</th>\n",
       "      <th>ODA4H2OPcptaDol</th>\n",
       "      <th>RenewResm3PcptaYr</th>\n",
       "      <th>SustAccImprWatRur</th>\n",
       "      <th>SustAccImprWatUrb</th>\n",
       "      <th>SustAccImprSanRur</th>\n",
       "      <th>SustAccImprSanUrb</th>\n",
       "      <th>TotHlthExpPctofGDP</th>\n",
       "      <th>GenGovtPctofTotHlthExp</th>\n",
       "      <th>ExtResHlthPctTotExpHlth</th>\n",
       "      <th>PCptaGovtExpHlthAvgExcRt</th>\n",
       "      <th>GDPPCptaIntDol</th>\n",
       "      <th>AdultLtrcyRate</th>\n",
       "      <th>FemaleLtrcyRate</th>\n",
       "      <th>BurdenOfDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2986</td>\n",
       "      <td>0.10891</td>\n",
       "      <td>0.18812</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.15842</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.4560</td>\n",
       "      <td>4</td>\n",
       "      <td>430</td>\n",
       "      <td>0.35644</td>\n",
       "      <td>0.20792</td>\n",
       "      <td>awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.58</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.94059</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.801980</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>49</td>\n",
       "      <td>6158</td>\n",
       "      <td>0.85644</td>\n",
       "      <td>0.78713</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>473</td>\n",
       "      <td>0.79208</td>\n",
       "      <td>0.91089</td>\n",
       "      <td>0.811880</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>71</td>\n",
       "      <td>4860</td>\n",
       "      <td>0.69307</td>\n",
       "      <td>0.60396</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  FrxnPeaceIn10  ODA4H2OPcptaDol  RenewResm3PcptaYr  \\\n",
       "0  Afghanistan            0.1             0.16               2986   \n",
       "1      Albania            1.0             5.58              13306   \n",
       "2      Algeria            0.0             0.33                473   \n",
       "\n",
       "   SustAccImprWatRur  SustAccImprWatUrb  SustAccImprSanRur  SustAccImprSanUrb  \\\n",
       "0            0.10891            0.18812           0.049505            0.15842   \n",
       "1            0.94059            0.98020           0.801980            0.98020   \n",
       "2            0.79208            0.91089           0.811880            0.98020   \n",
       "\n",
       "   TotHlthExpPctofGDP  GenGovtPctofTotHlthExp  ExtResHlthPctTotExpHlth  \\\n",
       "0               0.065                   0.395                   0.4560   \n",
       "1               0.065                   0.417                   0.0340   \n",
       "2               0.041                   0.808                   0.0005   \n",
       "\n",
       "   PCptaGovtExpHlthAvgExcRt  GDPPCptaIntDol  AdultLtrcyRate  FemaleLtrcyRate  \\\n",
       "0                         4             430         0.35644          0.20792   \n",
       "1                        49            6158         0.85644          0.78713   \n",
       "2                        71            4860         0.69307          0.60396   \n",
       "\n",
       "  BurdenOfDisease  \n",
       "0           awful  \n",
       "1             low  \n",
       "2            high  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Burden of diarrheal illness by country.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary\n",
    "\n",
    "NAME: Burden of diarrheal illness by country\n",
    "\n",
    "SIZE: 130 Countries, 16 Variables\n",
    "\n",
    "VARIABLE DESCRIPTIONS:\n",
    "\n",
    "Country: Country name\n",
    "\n",
    "FrxnPeaceIn10: Fraction of the past ten years in which a country has been at peace \n",
    "\n",
    "ODA4H2OPcptaDol: Per Capita Official Developmental Assistance for water projects\n",
    "\n",
    "RenewResm3PcptaYr: Renewable Water Resources in cubic meters per capita per year\n",
    "\n",
    "SustAccImprWatRur: Fraction of rural population with sustainable access to improved water\n",
    "\n",
    "SustAccImprWatUrb: Fraction of urban population with sustainable access to improved water\n",
    "\n",
    "SustAccImprSanRur: Fraction of rural population with sustainable access to improved sanitation\n",
    "\n",
    "SustAccImprSanUrb: Fraction of urban population with sustainable access to improved sanitation\n",
    "\n",
    "TotHlthExpPctofGDP: Fraction of a country's GDP devoted to health spending\n",
    "\n",
    "GenGovtPctofTotHlthExp: The fraction of total health expenditures for a country which is provided by the government\n",
    "\n",
    "ExtResHlthPctTotExpHlth: The fraction of total health expenditures for a country which is comes from sources external to the country\n",
    "\n",
    "PCptaGovtExpHlthAvgExcRt: Per Capita Government Health Expenditures at the average exchange rate\n",
    "\n",
    "GDPPCptaIntDol: Gross Domestic Product per capita in international dollars\n",
    "\n",
    "AdultLtrcyRate: Adult Literacy rate\n",
    "\n",
    "FemaleLtrcyRate: Female Literacy rate\n",
    "\n",
    "BurdenOfDisease: Our target variable for classification.  The burden of disease due to diarrheal illness, categorized into \"low\", \"medium\", \"high\", and \"awful\" quartiles.  For each country, we have estimates of the number of Disability-Adjusted Life Years lost per 1000 persons per year (DALYs) due to diarrheal illness.  Countries with \"low\" burden of disease have up to 2.75345 DALYs; countries with \"medium\" burden of disease have between 2.75345 and 8.2127 DALYs; countries with \"high\" burden of disease have between 8.2127 and 26.699 DALYs; and countries with \"awful\" burden of diease have more than 26.699 DALYs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your goal is to train a decision tree classifier for the attribute “BurdenOfDisease\" using all other variables (except country name) as features with sklearn.tree.DecisionTreeClassifier. \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Please choose a train/test split and choose a hyper-parameter governing model simplicity, for example, the maximum tree depth or maximum number of leaf nodes. Then, fit your decision tree classifier (using the training set) for different values of this parameter and for each such value, record the corresponding classification accuracy on the test set. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 130 entries, 0 to 129\n",
      "Data columns (total 16 columns):\n",
      "Country                     130 non-null object\n",
      "FrxnPeaceIn10               130 non-null float64\n",
      "ODA4H2OPcptaDol             130 non-null float64\n",
      "RenewResm3PcptaYr           130 non-null int64\n",
      "SustAccImprWatRur           130 non-null float64\n",
      "SustAccImprWatUrb           130 non-null float64\n",
      "SustAccImprSanRur           130 non-null float64\n",
      "SustAccImprSanUrb           130 non-null float64\n",
      "TotHlthExpPctofGDP          130 non-null float64\n",
      "GenGovtPctofTotHlthExp      130 non-null float64\n",
      "ExtResHlthPctTotExpHlth     130 non-null float64\n",
      "PCptaGovtExpHlthAvgExcRt    130 non-null int64\n",
      "GDPPCptaIntDol              130 non-null int64\n",
      "AdultLtrcyRate              130 non-null float64\n",
      "FemaleLtrcyRate             130 non-null float64\n",
      "BurdenOfDisease             130 non-null object\n",
      "dtypes: float64(11), int64(3), object(2)\n",
      "memory usage: 16.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with maximum depth 1 has an accuracy score of 0.423076923077\n",
      "Decision tree with maximum depth 2 has an accuracy score of 0.538461538462\n",
      "Decision tree with maximum depth 3 has an accuracy score of 0.615384615385\n",
      "Decision tree with maximum depth 4 has an accuracy score of 0.576923076923\n",
      "Decision tree with maximum depth 5 has an accuracy score of 0.576923076923\n",
      "Decision tree with maximum depth 6 has an accuracy score of 0.461538461538\n",
      "Decision tree with maximum depth 7 has an accuracy score of 0.538461538462\n",
      "Decision tree with maximum depth 8 has an accuracy score of 0.538461538462\n",
      "Decision tree with maximum depth 9 has an accuracy score of 0.538461538462\n",
      "Decision tree with maximum depth 10 has an accuracy score of 0.538461538462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# your code here\n",
    "\n",
    "# create the features space and the target variable\n",
    "X = data.iloc[:, 1:-1]\n",
    "y = data[['BurdenOfDisease']]\n",
    "\n",
    "# split data into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   stratify=y,\n",
    "                                                   random_state=1)\n",
    "\n",
    "# instantiate a decision tree, fit, predict and evaluate accuracy\n",
    "accuracy = []\n",
    "for i in range(1,11):\n",
    "    dt = DecisionTreeClassifier(max_depth=i, random_state=1)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracy.append(acc)\n",
    "    print 'Decision tree with maximum depth {} has an accuracy score of {}'.format(i, acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Make a plot of accuracy vs. simplicity for different values of the hyper-parameter chosen in part a). That is, the x-axis should be hyper-parameter value (e.g. tree depth) and the y-axis should be accuracy. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efeb5a2fdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The figure shows how the accuracy of a decision tree classifier changes when changing the maximum depth of the tree.\n",
      "The accuracy increased with depth to reach maximum accuracy of 0.615 at max depth 3.\n",
      "After that,the accuracy starts decreasing which is a sign of the model overfitting the training data to reach \n",
      "a minumum at maximum depth of 6. \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# your code here\n",
    "\n",
    "depth = range(1,11)\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(depth, accuracy)\n",
    "plt.xlabel('max depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy vs simplicity')\n",
    "plt.show()\n",
    "\n",
    "print 'The figure shows how the accuracy of a decision tree classifier changes when changing the maximum depth of the tree.\\\n",
    "\\nThe accuracy increased with depth to reach maximum accuracy of 0.615 at max depth 3.\\nAfter that,\\\n",
    "the accuracy starts decreasing which is a sign of the model overfitting the training data to reach \\\n",
    "\\na minumum at maximum depth of 6. '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Tune the hyper-parameter you choose in part a) by cross-validation using the training data. You can choose to use the GridSearchCV package from sklearn or write your own code to do cross-validation by spliting the training data into training and validation data. What is the out of sample accuracy after tuning the hyper-parameter? (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter: {'max_depth': 8}\n",
      "Best CV accuracy: 0.615384615385\n",
      "Out of sample accuracy: 0.538461538462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# your code here\n",
    "\n",
    "# instantiate a decision tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# define the hyperparameter space\n",
    "params_dt = {'max_depth': depth}\n",
    "\n",
    "# instantiate a 10-fold CV grid search\n",
    "grid_dt = GridSearchCV(estimator=dt, param_grid=params_dt,\n",
    "                      scoring='accuracy', cv=10)\n",
    "\n",
    "# fit on the training data\n",
    "grid_dt.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# extract best hyperparameter\n",
    "best_hyperparameter = grid_dt.best_params_\n",
    "print 'Best hyperparameter:', best_hyperparameter\n",
    "\n",
    "# extract best cv score \n",
    "best_cv_score = grid_dt.best_score_\n",
    "print 'Best CV accuracy:', best_cv_score\n",
    "\n",
    "# extract best model\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# evaluate on out of sample\n",
    "out_of_sample_acc = best_model.score(X_test, y_test)\n",
    "print 'Out of sample accuracy:', out_of_sample_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53846153846153844"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.490385</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>{u'max_depth': 1}</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.026624</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.750951</td>\n",
       "      <td>2</td>\n",
       "      <td>{u'max_depth': 2}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.123761</td>\n",
       "      <td>0.017590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.832563</td>\n",
       "      <td>3</td>\n",
       "      <td>{u'max_depth': 3}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.101911</td>\n",
       "      <td>0.033115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.911549</td>\n",
       "      <td>4</td>\n",
       "      <td>{u'max_depth': 4}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.118286</td>\n",
       "      <td>0.022213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>0.961549</td>\n",
       "      <td>5</td>\n",
       "      <td>{u'max_depth': 5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.967391</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.927083</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.146141</td>\n",
       "      <td>0.020260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.979574</td>\n",
       "      <td>6</td>\n",
       "      <td>{u'max_depth': 6}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.153646</td>\n",
       "      <td>0.014964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010972</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.990263</td>\n",
       "      <td>7</td>\n",
       "      <td>{u'max_depth': 7}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.027909</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.155143</td>\n",
       "      <td>0.011345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>8</td>\n",
       "      <td>{u'max_depth': 8}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.150811</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.998913</td>\n",
       "      <td>9</td>\n",
       "      <td>{u'max_depth': 9}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>0.003261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>{u'max_depth': 10}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.173255</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.001400         0.000401         0.490385          0.500000   \n",
       "1       0.001404         0.000369         0.576923          0.750951   \n",
       "2       0.001505         0.000373         0.596154          0.832563   \n",
       "3       0.001573         0.000376         0.596154          0.911549   \n",
       "4       0.001650         0.000372         0.586538          0.961549   \n",
       "5       0.001654         0.000371         0.596154          0.979574   \n",
       "6       0.010972         0.000386         0.605769          0.990263   \n",
       "7       0.001659         0.000374         0.615385          0.995652   \n",
       "8       0.001663         0.000372         0.596154          0.998913   \n",
       "9       0.001661         0.000372         0.596154          1.000000   \n",
       "\n",
       "  param_max_depth              params  rank_test_score  split0_test_score  \\\n",
       "0               1   {u'max_depth': 1}               10           0.500000   \n",
       "1               2   {u'max_depth': 2}                9           0.500000   \n",
       "2               3   {u'max_depth': 3}                3           0.666667   \n",
       "3               4   {u'max_depth': 4}                3           0.416667   \n",
       "4               5   {u'max_depth': 5}                8           0.333333   \n",
       "5               6   {u'max_depth': 6}                3           0.333333   \n",
       "6               7   {u'max_depth': 7}                2           0.333333   \n",
       "7               8   {u'max_depth': 8}                1           0.333333   \n",
       "8               9   {u'max_depth': 9}                3           0.333333   \n",
       "9              10  {u'max_depth': 10}                3           0.333333   \n",
       "\n",
       "   split0_train_score  split1_test_score       ...         split7_test_score  \\\n",
       "0            0.500000           0.500000       ...                     0.500   \n",
       "1            0.760870           0.583333       ...                     0.500   \n",
       "2            0.847826           0.583333       ...                     0.500   \n",
       "3            0.934783           0.583333       ...                     0.500   \n",
       "4            0.967391           0.583333       ...                     0.625   \n",
       "5            0.989130           0.583333       ...                     0.625   \n",
       "6            1.000000           0.666667       ...                     0.625   \n",
       "7            1.000000           0.583333       ...                     0.625   \n",
       "8            1.000000           0.583333       ...                     0.625   \n",
       "9            1.000000           0.583333       ...                     0.625   \n",
       "\n",
       "   split7_train_score  split8_test_score  split8_train_score  \\\n",
       "0            0.500000              0.500            0.500000   \n",
       "1            0.770833              0.500            0.770833   \n",
       "2            0.843750              0.500            0.843750   \n",
       "3            0.937500              0.625            0.906250   \n",
       "4            0.989583              0.500            0.968750   \n",
       "5            1.000000              0.625            0.989583   \n",
       "6            1.000000              0.625            1.000000   \n",
       "7            1.000000              0.625            1.000000   \n",
       "8            1.000000              0.625            1.000000   \n",
       "9            1.000000              0.625            1.000000   \n",
       "\n",
       "   split9_test_score  split9_train_score  std_fit_time  std_score_time  \\\n",
       "0              0.500            0.500000      0.000186        0.000042   \n",
       "1              0.750            0.750000      0.000036        0.000005   \n",
       "2              0.750            0.822917      0.000038        0.000006   \n",
       "3              0.750            0.875000      0.000051        0.000008   \n",
       "4              0.625            0.927083      0.000067        0.000004   \n",
       "5              0.750            0.979167      0.000039        0.000002   \n",
       "6              0.750            0.989583      0.027909        0.000031   \n",
       "7              0.750            1.000000      0.000028        0.000005   \n",
       "8              0.750            1.000000      0.000038        0.000004   \n",
       "9              0.750            1.000000      0.000034        0.000005   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.026624         0.000000  \n",
       "1        0.123761         0.017590  \n",
       "2        0.101911         0.033115  \n",
       "3        0.118286         0.022213  \n",
       "4        0.146141         0.020260  \n",
       "5        0.153646         0.014964  \n",
       "6        0.155143         0.011345  \n",
       "7        0.150811         0.008696  \n",
       "8        0.173255         0.003261  \n",
       "9        0.173255         0.000000  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_dt.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Visualize a simple decision tree (e.g., with max_depth = 2 or 3) learned from the data.  To do so, given your decision tree dt, you can use the code below, then copy and paste the resulting output into http://www.webgraphviz.com.  Alternatively, if you have graphviz installed on your machine, you can use that. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph Tree {\n",
      "node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=helvetica] ;\n",
      "edge [fontname=helvetica] ;\n",
      "0 [label=\"GDPPCptaIntDol <= 2978.5, samples = 104, value = [26, 26, 26, 26], class = awful\", fillcolor=\"#e5813900\"] ;\n",
      "1 [label=\"SustAccImprSanUrb <= 0.6634, samples = 46, value = [26, 19, 0, 1], class = awful\", fillcolor=\"#e5813942\"] ;\n",
      "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
      "2 [label=\"samples = 32, value = [25, 7, 0, 0], class = awful\", fillcolor=\"#e58139b8\"] ;\n",
      "1 -> 2 ;\n",
      "3 [label=\"samples = 14, value = [1, 12, 0, 1], class = high\", fillcolor=\"#47e539d8\"] ;\n",
      "1 -> 3 ;\n",
      "4 [label=\"GDPPCptaIntDol <= 9195.0, samples = 58, value = [0, 7, 26, 25], class = low\", fillcolor=\"#399de508\"] ;\n",
      "0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
      "5 [label=\"samples = 40, value = [0, 7, 9, 24], class = medium\", fillcolor=\"#d739e57b\"] ;\n",
      "4 -> 5 ;\n",
      "6 [label=\"samples = 18, value = [0, 0, 17, 1], class = low\", fillcolor=\"#399de5f0\"] ;\n",
      "4 -> 6 ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# your code here\n",
    "dt = DecisionTreeClassifier(max_depth=2)\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "thestring=tree.export_graphviz(dt,out_file=None,\n",
    "                         feature_names=X_train.columns.values,  \n",
    "                         class_names=dt.classes_,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False).replace(\"<br/>\",\", \").replace(\"&le;\",\"<=\").replace(\"=<\",\"=\\\"\").replace(\">,\",\"\\\",\")\n",
    "print thestring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please make sure you dowloaded the image (tree2.jpg) and have it in the same directory with this notebook to be able to view the image below.**\n",
    "![alt text](tree2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4, Fit a random forest to the data from question 3 (20 pts)\n",
    "\n",
    "a) Please use the same test/train split from previous question and feel free to tune the hyper-parameters for Random Forest model using training data. The package from sklearn is here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.\n",
    "Then please report your out of sample prediction result and compare this model's performance with 3c). (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# your code here\n",
    "\n",
    "# instantiate a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_split': 1e-07,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the random forest's hyperparamters\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams:\n",
      "{'max_features': 'log2', 'n_estimators': 20, 'max_depth': 2, 'min_samples_leaf': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# define a grid of hyperparameters to tune on\n",
    "params_rf = {'n_estimators':range(20, 80, 10),\n",
    "            'max_depth': depth,\n",
    "            'min_samples_leaf': [0.1, 0.2],\n",
    "            'max_features': ['log2', 'sqrt']}\n",
    "\n",
    "# instantiate a grid search\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                      param_grid=params_rf,\n",
    "                      cv=3,\n",
    "                      scoring='accuracy',\n",
    "                      n_jobs=-1)\n",
    "\n",
    "# fit grid_rf to the training data\n",
    "grid_rf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# extract the best hyperparameters\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print 'Best hyperparams:\\n', best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of sample accuracy: 0.576923076923\n"
     ]
    }
   ],
   "source": [
    "# extract the best model\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# evaluate out of sample accuracy\n",
    "out_of_sample_acc = best_model.score(X_test, y_test)\n",
    "print 'Out of sample accuracy:', out_of_sample_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model performance slightly enhanced when we used random forests.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Write one paragraph comparing the results from those two models (Random Forest vs Decision Tree) in terms of both accuracy and interpretability. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Using a random forest algorithm to predict the burden of diarrheal illness resulted in a slightly better performing model (with an accuracy of 0.58) than a decision tree algorithm (with an accuracy of 0.54). The random forest was based on 20 estimators/trees versus one tree in a decision tree model. The use of multiple trees makes the random forest less interpretable yet a bit more accurate. However, given this slight increase in accuracy, one must asses the nature of the problem in hand and decide if accuracy is more important than interpretability. In medical applications, even a slight increase in accuracy may outweigh the benefits of explanation.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
